{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f8c09a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ce notebook est consacré à la réalisation d'un modèle de régression basé sur le LGBMRegressor (Light Gradient Boosting Machine), afin de prédire les ventes d'un ensemble de magasins. Pour cela, diverses données sont prises en compte : \n",
    "\n",
    "1. Les informations générales concernant les ventes et les magasins (fichier `train.csv`, `test.csv` et `stores.csv`), \n",
    "2. Les variations des prix du pétrole (`oil.csv`), facteur susceptible d'influencer le pouvoir d'achat des consommateurs,\n",
    "3. Les jours fériés et événements spéciaux (`holidays_events.csv`) qui peuvent affecter les comportements d'achat,\n",
    "4. Les transactions effectuées (`transactions.csv`).\n",
    "\n",
    "Après avoir importé ces données, le notebook procède à une série de pré-traitements, incluant la fusion des différentes sources d'informations, le remplissage des valeurs manquantes, et la factorisation des données catégorielles. De plus, des caractéristiques temporelles supplémentaires sont ajoutées pour enrichir le modèle.\n",
    "\n",
    "Une fois ces étapes de préparation effectuées, les données sont divisées en un ensemble d'entraînement et un ensemble de test, et un modèle LGBMRegressor est entraîné sur ces données. Les prédictions sont ensuite générées et évaluées à l'aide de l'erreur quadratique moyenne logarithmique. Enfin, les prédictions négatives sont ajustées à zéro, car une prédiction de vente ne peut être négative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d25e4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1966285",
   "metadata": {},
   "source": [
    "Ces lignes de code servent à importer plusieurs jeux de données à partir de fichiers CSV et à préparer ces données pour une analyse ultérieure.\n",
    "\n",
    "1. `holidays = pd.read_csv('holidays_events.csv', parse_dates=True)`: Importe les données des événements spéciaux et des jours fériés, en s'assurant que toutes les dates sont correctement analysées comme des objets datetime.\n",
    "\n",
    "2. `oil = pd.read_csv('oil.csv', parse_dates=True, index_col=0)`: Importe les données sur les prix de l'huile, tout en fixant la première colonne (qui est la date) comme l'index du DataFrame.\n",
    "\n",
    "3. `sample_submission = pd.read_csv('sample_submission.csv')`: Importe le modèle de soumission qui est utilisé pour formater correctement nos prédictions avant de les soumettre pour évaluation.\n",
    "\n",
    "4. `stores = pd.read_csv('stores.csv')`: Importe les informations détaillées sur chaque magasin.\n",
    "\n",
    "5. `data_test = pd.read_csv('test.csv')` et `data_train = pd.read_csv('train.csv')`: Importent respectivement les données de test et d'entraînement. Ces données comprennent des informations détaillées sur chaque transaction.\n",
    "\n",
    "6. `transactions = pd.read_csv('transactions.csv')`: Importe les informations sur toutes les transactions effectuées.\n",
    "\n",
    "7. `data_test['date'] = pd.to_datetime(data_test.date)`, `data_train['date'] = pd.to_datetime(data_train.date)`, et `holidays['date'] = pd.to_datetime(holidays.date)`: Convertissent la colonne de date dans les DataFrames de test, d'entraînement et des jours fériés en datetime. Cela facilite le traitement ultérieur des dates et des heures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fc90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.read_csv('holidays_events.csv', parse_dates=True)\n",
    "oil = pd.read_csv('oil.csv', parse_dates=True, index_col=0)\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train = pd.read_csv('train.csv')\n",
    "transactions = pd.read_csv('transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b1768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['date'] = pd.to_datetime(data_test.date)\n",
    "data_train['date'] = pd.to_datetime(data_train.date)\n",
    "holidays['date'] = pd.to_datetime(holidays.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9235ec8",
   "metadata": {},
   "source": [
    "Cette portion de code effectue plusieurs opérations de prétraitement sur les données :\n",
    "\n",
    "1. **Création de l'index 'oil' :** Elle commence par la création d'un nouvel index pour le DataFrame 'oil'. Cet index est une combinaison des dates uniques présentes à la fois dans les DataFrames 'data_train' et 'data_test'. Les valeurs manquantes dans l'index 'oil' sont ensuite remplies en utilisant la méthode 'forward fill' et 'backward fill'.\n",
    "\n",
    "2. **Fusion des DataFrames :** Les DataFrames 'data_train' et 'data_test' sont ensuite enrichis en fusionnant avec les DataFrames 'oil', 'stores', et 'holidays' sur les colonnes appropriées. La méthode 'left' est utilisée, ce qui signifie que toutes les lignes du DataFrame 'data_train' et 'data_test' sont conservées et seulement les lignes correspondantes des autres DataFrames sont utilisées.\n",
    "\n",
    "3. **Traitement des valeurs manquantes :** Ensuite, toutes les colonnes présentant des valeurs manquantes dans 'data_test' sont identifiées. Ces valeurs manquantes sont remplies avec la valeur 'None' à la fois dans 'data_train' et 'data_test'.\n",
    "\n",
    "4. **Factorisation des données :** Les DataFrames 'data_train' et 'data_test' sont ensuite concaténés en un seul DataFrame 'data_factorize', et toutes les colonnes de type 'object' sont factorisées. Cela signifie que les chaînes de caractères uniques sont remplacées par des nombres entiers, ce qui est souvent nécessaire pour les algorithmes de machine learning.\n",
    "\n",
    "5. **Séparation des données d'entraînement et de test :** Le DataFrame 'data_factorize' est ensuite divisé à nouveau en 'data_train' et 'data_test' en utilisant l'index précédemment stocké ('test_idx').\n",
    "\n",
    "6. **Ajout de caractéristiques temporelles :** Enfin, la fonction 'add_time_cols' est définie et utilisée pour ajouter des caractéristiques temporelles supplémentaires ('day', 'month', 'year') aux DataFrames 'data_train' et 'data_test'. La colonne 'date' est ensuite supprimée de ces DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cadb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_oil = pd.Index((pd.concat([pd.Series(data_train.date.unique()), pd.Series(data_test.date.unique())], axis=0)\n",
    "                     .reset_index()\n",
    "                     .drop(['index'], axis=1))[0])\n",
    "index_oil.name = 'date'\n",
    "oil = oil.reindex(index_oil).reset_index()\n",
    "oil = oil.fillna(method='ffill').fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9594e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.merge(data_train, oil.reset_index(), on='date', how='left').drop(['index'], axis=1)\n",
    "data_test = pd.merge(data_test, oil.reset_index(), on='date', how='left').drop(['index'], axis=1)\n",
    "data_train = pd.merge(data_train, stores, on='store_nbr', how='left')\n",
    "data_test = pd.merge(data_test, stores, on='store_nbr', how='left')\n",
    "data_train = pd.merge(data_train, holidays.drop_duplicates(subset='date'), on='date', how='left')\n",
    "data_test = pd.merge(data_test, holidays.drop_duplicates(subset='date'), on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3e90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.set_index('id')\n",
    "data_test = data_test.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed636b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_col = [c for c in data_test.isna().any()[data_test.isna().any()].index]\n",
    "data_train[na_col] = data_train[na_col].fillna(value='None')\n",
    "data_test[na_col] = data_test[na_col].fillna(value='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "982bcbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = data_test.index[0]\n",
    "data_factorize = pd.concat([data_train, data_test], axis=0)\n",
    "for col, types in zip(data_factorize, data_factorize.dtypes):\n",
    "    if types == 'object':\n",
    "        data_factorize[col] = pd.factorize(data_factorize[col])[0]\n",
    "        \n",
    "data_test = data_factorize.iloc[test_idx:].drop(['sales'], axis=1)\n",
    "data_train = data_factorize.iloc[:test_idx]\n",
    "del data_factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dfaaf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_cols(data):\n",
    "    data['day'] = data['date'].dt.dayofweek\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data.drop(['date'], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "data_train, data_test = add_time_cols(data_train), add_time_cols(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2c7ce",
   "metadata": {},
   "source": [
    "Cette portion de code correspond à l'entraînement et à l'évaluation du modèle de régression :\n",
    "\n",
    "1. **Séparation des caractéristiques et de la cible :** Les données d'entraînement sont divisées en caractéristiques (X) et cible (y). La cible est la colonne 'sales', que le modèle cherchera à prédire.\n",
    "\n",
    "2. **Division des données en ensembles d'entraînement et de test :** Les données sont ensuite divisées en un ensemble d'entraînement et un ensemble de test en utilisant la fonction `train_test_split` de scikit-learn. 80% des données sont utilisées pour l'entraînement du modèle, et les 20% restants sont utilisés pour tester ses performances. L'option `shuffle=False` garantit que les données ne sont pas mélangées avant la division, ce qui est important pour conserver la structure temporelle des données.\n",
    "\n",
    "3. **Entraînement du modèle :** Un modèle de régression LightGBM est ensuite entraîné sur l'ensemble d'entraînement. L'option `n_jobs=-1` permet d'utiliser tous les cœurs du processeur pour l'entraînement.\n",
    "\n",
    "4. **Prédiction et évaluation :** Le modèle est utilisé pour prédire les ventes sur l'ensemble de test. Les prédictions négatives sont ajustées à 0, car une prédiction de vente ne peut être négative. Enfin, l'erreur quadratique moyenne logarithmique (mean squared log error) entre les valeurs prédites et les valeurs réelles est calculée et affichée.\n",
    "\n",
    "5. **Vérification des prédictions :** Enfin, la valeur minimale des prédictions est calculée et affichée. Cela est probablement fait pour vérifier que les prédictions négatives ont bien été ajustées à 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96cfb3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_train.drop(['sales'], axis=1), data_train['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddaeca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79349fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.118522077604036\n"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor(n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "preds = np.where(preds < 0, 0, preds)\n",
    "print(mean_squared_log_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a73be451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.5684359719774"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(preds < 0, 0, preds).min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
